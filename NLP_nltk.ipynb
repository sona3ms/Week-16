{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9eMmFrPBMHK",
        "outputId": "5cf067d3-28af-4eb5-d763-fae8338679ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVbuM8KmHLJO",
        "outputId": "da4da472-7411-4ee7-bb32-4f37f1079d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"This is a sample sentence for tokenization.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai1FyLjrHRML",
        "outputId": "3d422e6f-3ae5-4e2c-c655-b0fffc784b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#paragraph to senetence"
      ],
      "metadata": {
        "id": "czIoMjtBH8xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "paragraph = \"\"\"\n",
        "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. NLP techniques enable computers to understand, interpret, and generate human language in a way that is both valuable and meaningful. It has various applications, including chatbots, sentiment analysis, machine translation, and text summarization. NLTK (Natural Language Toolkit) is a Python library that provides tools for working with human language data. Let's use NLTK to tokenize this paragraph into sentences. NLTK is a powerful tool for NLP tasks, and it makes working with text data easier and more efficient.\n",
        "\"\"\"\n",
        "sentence = sent_tokenize(paragraph)\n",
        "paragraph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "K7iFSDNTHWhH",
        "outputId": "8f7e355c-0159-4735-dc9a-3fe6c92eafd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nNatural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. NLP techniques enable computers to understand, interpret, and generate human language in a way that is both valuable and meaningful. It has various applications, including chatbots, sentiment analysis, machine translation, and text summarization. NLTK (Natural Language Toolkit) is a Python library that provides tools for working with human language data. Let's use NLTK to tokenize this paragraph into sentences. NLTK is a powerful tool for NLP tasks, and it makes working with text data easier and more efficient.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8f-LsH0Ilfh",
        "outputId": "447b3340-3615-4a7f-daaa-b3f552e4c093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nNatural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.',\n",
              " 'NLP techniques enable computers to understand, interpret, and generate human language in a way that is both valuable and meaningful.',\n",
              " 'It has various applications, including chatbots, sentiment analysis, machine translation, and text summarization.',\n",
              " 'NLTK (Natural Language Toolkit) is a Python library that provides tools for working with human language data.',\n",
              " \"Let's use NLTK to tokenize this paragraph into sentences.\",\n",
              " 'NLTK is a powerful tool for NLP tasks, and it makes working with text data easier and more efficient.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentence,1):\n",
        "  print(f\"sentence{i}:{sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsoMUZ4LI7Hl",
        "outputId": "d55f0850-07d9-48c0-e212-2e8faccb098f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence1:\n",
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
            "sentence2:NLP techniques enable computers to understand, interpret, and generate human language in a way that is both valuable and meaningful.\n",
            "sentence3:It has various applications, including chatbots, sentiment analysis, machine translation, and text summarization.\n",
            "sentence4:NLTK (Natural Language Toolkit) is a Python library that provides tools for working with human language data.\n",
            "sentence5:Let's use NLTK to tokenize this paragraph into sentences.\n",
            "sentence6:NLTK is a powerful tool for NLP tasks, and it makes working with text data easier and more efficient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word = word_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "L65eWpcRJVcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, word in enumerate(word,1):\n",
        "  print(f\"word{i}:{word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nowAYJmKlqj",
        "outputId": "6c791931-bbb1-4863-c0c9-6d1c8ffe6edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word1:Natural\n",
            "word2:Language\n",
            "word3:Processing\n",
            "word4:(\n",
            "word5:NLP\n",
            "word6:)\n",
            "word7:is\n",
            "word8:a\n",
            "word9:field\n",
            "word10:of\n",
            "word11:artificial\n",
            "word12:intelligence\n",
            "word13:that\n",
            "word14:focuses\n",
            "word15:on\n",
            "word16:the\n",
            "word17:interaction\n",
            "word18:between\n",
            "word19:computers\n",
            "word20:and\n",
            "word21:humans\n",
            "word22:through\n",
            "word23:natural\n",
            "word24:language\n",
            "word25:.\n",
            "word26:NLP\n",
            "word27:techniques\n",
            "word28:enable\n",
            "word29:computers\n",
            "word30:to\n",
            "word31:understand\n",
            "word32:,\n",
            "word33:interpret\n",
            "word34:,\n",
            "word35:and\n",
            "word36:generate\n",
            "word37:human\n",
            "word38:language\n",
            "word39:in\n",
            "word40:a\n",
            "word41:way\n",
            "word42:that\n",
            "word43:is\n",
            "word44:both\n",
            "word45:valuable\n",
            "word46:and\n",
            "word47:meaningful\n",
            "word48:.\n",
            "word49:It\n",
            "word50:has\n",
            "word51:various\n",
            "word52:applications\n",
            "word53:,\n",
            "word54:including\n",
            "word55:chatbots\n",
            "word56:,\n",
            "word57:sentiment\n",
            "word58:analysis\n",
            "word59:,\n",
            "word60:machine\n",
            "word61:translation\n",
            "word62:,\n",
            "word63:and\n",
            "word64:text\n",
            "word65:summarization\n",
            "word66:.\n",
            "word67:NLTK\n",
            "word68:(\n",
            "word69:Natural\n",
            "word70:Language\n",
            "word71:Toolkit\n",
            "word72:)\n",
            "word73:is\n",
            "word74:a\n",
            "word75:Python\n",
            "word76:library\n",
            "word77:that\n",
            "word78:provides\n",
            "word79:tools\n",
            "word80:for\n",
            "word81:working\n",
            "word82:with\n",
            "word83:human\n",
            "word84:language\n",
            "word85:data\n",
            "word86:.\n",
            "word87:Let\n",
            "word88:'s\n",
            "word89:use\n",
            "word90:NLTK\n",
            "word91:to\n",
            "word92:tokenize\n",
            "word93:this\n",
            "word94:paragraph\n",
            "word95:into\n",
            "word96:sentences\n",
            "word97:.\n",
            "word98:NLTK\n",
            "word99:is\n",
            "word100:a\n",
            "word101:powerful\n",
            "word102:tool\n",
            "word103:for\n",
            "word104:NLP\n",
            "word105:tasks\n",
            "word106:,\n",
            "word107:and\n",
            "word108:it\n",
            "word109:makes\n",
            "word110:working\n",
            "word111:with\n",
            "word112:text\n",
            "word113:data\n",
            "word114:easier\n",
            "word115:and\n",
            "word116:more\n",
            "word117:efficient\n",
            "word118:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word = word_tokenize(sentence)\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0QL6KZqKmv1",
        "outputId": "37573448-bd7d-4014-ba87-f08dba0973a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'NLP', 'tasks', ',', 'and', 'it', 'makes', 'working', 'with', 'text', 'data', 'easier', 'and', 'more', 'efficient', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming"
      ],
      "metadata": {
        "id": "QSieVB-BXt6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer,SnowballStemmer,LancasterStemmer\n",
        "P = PorterStemmer()\n",
        "S = SnowballStemmer(\"english\")# Snowball stemmer for English\n",
        "L = LancasterStemmer()\n",
        "words = [\"running\", \"flies\", \"jumping\", \"jumps\", \"easily\"]\n",
        "P_w =[P.stem(i) for i in words]\n",
        "S_w =[S.stem(i) for i in words]\n",
        "l_w =[L.stem(i) for i in words]\n",
        "print(words)\n",
        "print(P_w)\n",
        "print(S_w)\n",
        "print(l_w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib96j7L5L18A",
        "outputId": "ea1fd389-432d-4cda-ae57-3bb58b18bd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'flies', 'jumping', 'jumps', 'easily']\n",
            "['run', 'fli', 'jump', 'jump', 'easili']\n",
            "['run', 'fli', 'jump', 'jump', 'easili']\n",
            "['run', 'fli', 'jump', 'jump', 'easy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = word_tokenize(paragraph)\n",
        "stemmed_paragraph = [P.stem(i) for i in word]\n",
        "stemmed_paragraph_text = ' '.join(stemmed_paragraph)\n",
        "print(\"Original Paragraph:\")\n",
        "print(paragraph)\n",
        "print(\"\\nStemmed Paragraph:\")\n",
        "print(stemmed_paragraph)\n",
        "print(\"\\nStemmed Paragraph:\")\n",
        "print(stemmed_paragraph_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvbTOBOCUoWY",
        "outputId": "6a97681c-68fa-4ada-b601-1b633ae935e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            "\n",
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. NLP techniques enable computers to understand, interpret, and generate human language in a way that is both valuable and meaningful. It has various applications, including chatbots, sentiment analysis, machine translation, and text summarization. NLTK (Natural Language Toolkit) is a Python library that provides tools for working with human language data. Let's use NLTK to tokenize this paragraph into sentences. NLTK is a powerful tool for NLP tasks, and it makes working with text data easier and more efficient.\n",
            "\n",
            "\n",
            "Stemmed Paragraph:\n",
            "['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'artifici', 'intellig', 'that', 'focus', 'on', 'the', 'interact', 'between', 'comput', 'and', 'human', 'through', 'natur', 'languag', '.', 'nlp', 'techniqu', 'enabl', 'comput', 'to', 'understand', ',', 'interpret', ',', 'and', 'gener', 'human', 'languag', 'in', 'a', 'way', 'that', 'is', 'both', 'valuabl', 'and', 'meaning', '.', 'it', 'ha', 'variou', 'applic', ',', 'includ', 'chatbot', ',', 'sentiment', 'analysi', ',', 'machin', 'translat', ',', 'and', 'text', 'summar', '.', 'nltk', '(', 'natur', 'languag', 'toolkit', ')', 'is', 'a', 'python', 'librari', 'that', 'provid', 'tool', 'for', 'work', 'with', 'human', 'languag', 'data', '.', 'let', \"'s\", 'use', 'nltk', 'to', 'token', 'thi', 'paragraph', 'into', 'sentenc', '.', 'nltk', 'is', 'a', 'power', 'tool', 'for', 'nlp', 'task', ',', 'and', 'it', 'make', 'work', 'with', 'text', 'data', 'easier', 'and', 'more', 'effici', '.']\n",
            "\n",
            "Stemmed Paragraph:\n",
            "natur languag process ( nlp ) is a field of artifici intellig that focus on the interact between comput and human through natur languag . nlp techniqu enabl comput to understand , interpret , and gener human languag in a way that is both valuabl and meaning . it ha variou applic , includ chatbot , sentiment analysi , machin translat , and text summar . nltk ( natur languag toolkit ) is a python librari that provid tool for work with human languag data . let 's use nltk to token thi paragraph into sentenc . nltk is a power tool for nlp task , and it make work with text data easier and more effici .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lemmatization"
      ],
      "metadata": {
        "id": "OMOM_EniX1lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrXVIZcfXCXO",
        "outputId": "49a01b52-0116-455c-cc72-7ab5e036d5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'flies', 'jumping', 'jumps', 'easily']\n",
            "Lemmatized Words: ['running', 'fly', 'jumping', 'jump', 'easily']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word = word_tokenize(paragraph)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in word]\n",
        "l_paragraph_text = ' '.join(lemmatized_words)\n",
        "print(\"Original Words:\", word)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n",
        "print(\"Lemmatized Words:\",l_paragraph_text )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR7hD0tfYknP",
        "outputId": "117b7dbf-ad8f-49a8-da1a-ef3295cbf86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'NLP', 'techniques', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'valuable', 'and', 'meaningful', '.', 'It', 'has', 'various', 'applications', ',', 'including', 'chatbots', ',', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'and', 'text', 'summarization', '.', 'NLTK', '(', 'Natural', 'Language', 'Toolkit', ')', 'is', 'a', 'Python', 'library', 'that', 'provides', 'tools', 'for', 'working', 'with', 'human', 'language', 'data', '.', 'Let', \"'s\", 'use', 'NLTK', 'to', 'tokenize', 'this', 'paragraph', 'into', 'sentences', '.', 'NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'NLP', 'tasks', ',', 'and', 'it', 'makes', 'working', 'with', 'text', 'data', 'easier', 'and', 'more', 'efficient', '.']\n",
            "Lemmatized Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focus', 'on', 'the', 'interaction', 'between', 'computer', 'and', 'human', 'through', 'natural', 'language', '.', 'NLP', 'technique', 'enable', 'computer', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'valuable', 'and', 'meaningful', '.', 'It', 'ha', 'various', 'application', ',', 'including', 'chatbots', ',', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'and', 'text', 'summarization', '.', 'NLTK', '(', 'Natural', 'Language', 'Toolkit', ')', 'is', 'a', 'Python', 'library', 'that', 'provides', 'tool', 'for', 'working', 'with', 'human', 'language', 'data', '.', 'Let', \"'s\", 'use', 'NLTK', 'to', 'tokenize', 'this', 'paragraph', 'into', 'sentence', '.', 'NLTK', 'is', 'a', 'powerful', 'tool', 'for', 'NLP', 'task', ',', 'and', 'it', 'make', 'working', 'with', 'text', 'data', 'easier', 'and', 'more', 'efficient', '.']\n",
            "Lemmatized Words: Natural Language Processing ( NLP ) is a field of artificial intelligence that focus on the interaction between computer and human through natural language . NLP technique enable computer to understand , interpret , and generate human language in a way that is both valuable and meaningful . It ha various application , including chatbots , sentiment analysis , machine translation , and text summarization . NLTK ( Natural Language Toolkit ) is a Python library that provides tool for working with human language data . Let 's use NLTK to tokenize this paragraph into sentence . NLTK is a powerful tool for NLP task , and it make working with text data easier and more efficient .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords"
      ],
      "metadata": {
        "id": "vjqo-l-qENdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"This is an example sentence with some stopwords that we want to remove.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Remove stopwords from the tokenized words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Join the filtered words to form a clean sentence\n",
        "clean_sentence = ' '.join(filtered_words)\n",
        "\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"Cleaned Sentence:\", clean_sentence)\n"
      ],
      "metadata": {
        "id": "xbiM0shNYw6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "284fa558-0833-48b9-d665-42d7fb4f0a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: This is an example sentence with some stopwords that we want to remove.\n",
            "Cleaned Sentence: example sentence stopwords want remove .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = nltk.sent_tokenize(paragraph)\n",
        "from nltk.corpus import stopwords\n",
        "for i in range(len(s)):\n",
        "  words = nltk.word_tokenize(s[i])\n",
        "  words = [i for i in words if i.lower() not in set(stopwords.words('english'))]\n",
        "  s[i]=' '.join(words)\n",
        "for sentence in s:\n",
        "   print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q22d_ASaE4Hn",
        "outputId": "561576d5-2cac-40ac-a2d5-fa996e2ec68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing ( NLP ) field artificial intelligence focuses interaction computers humans natural language .\n",
            "NLP techniques enable computers understand , interpret , generate human language way valuable meaningful .\n",
            "various applications , including chatbots , sentiment analysis , machine translation , text summarization .\n",
            "NLTK ( Natural Language Toolkit ) Python library provides tools working human language data .\n",
            "Let 's use NLTK tokenize paragraph sentences .\n",
            "NLTK powerful tool NLP tasks , makes working text data easier efficient .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#N-grams - Spacy"
      ],
      "metadata": {
        "id": "SZTuDh7BXV5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"This is a sample sentence for N-gram implementation using spaCy and NLTK.\"\n",
        "doc=nlp(text)"
      ],
      "metadata": {
        "id": "4jYx655pQ6wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 2\n",
        "ngrams = [doc[i:i + n]for i in range(len(doc)-n+1)]\n",
        "for gram in ngrams:\n",
        "    print(gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TdaYsP402kZ",
        "outputId": "308ff4c1-cc90-4228-f922-f5e07f6da377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is\n",
            "is a\n",
            "a sample\n",
            "sample sentence\n",
            "sentence for\n",
            "for N\n",
            "N-\n",
            "-gram\n",
            "gram implementation\n",
            "implementation using\n",
            "using spaCy\n",
            "spaCy and\n",
            "and NLTK\n",
            "NLTK.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_spacy_ngrams(text, n):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc=nlp(text)\n",
        "  return [doc[i:i + n]for i in range(len(doc)-n+1)]\n",
        "\n",
        "n=4\n",
        "text = \"This is a sample sentence for N-gram implementation using spaCy and NLTK.\"\n",
        "ngram_list = generate_spacy_ngrams(text, n)\n",
        "for gram in ngram_list:\n",
        "    print(gram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_ZquCFP1ESS",
        "outputId": "10a55830-be86-489b-fe80-101bf67af84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample\n",
            "is a sample sentence\n",
            "a sample sentence for\n",
            "sample sentence for N\n",
            "sentence for N-\n",
            "for N-gram\n",
            "N-gram implementation\n",
            "-gram implementation using\n",
            "gram implementation using spaCy\n",
            "implementation using spaCy and\n",
            "using spaCy and NLTK\n",
            "spaCy and NLTK.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import spacy\n",
        "def generate_spacy_ngrams(text, n):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc=nlp(text)\n",
        "  char_remove = string.punctuation\n",
        "  tokens =[token.text.lower() for token in doc if token.text not in char_remove]\n",
        "  return [doc[i:i + n]for i in range(len(doc)-n+1)]\n",
        "\n",
        "n=4\n",
        "text = \"This is a sample sentence for N-gram implementation using spaCy and NLTK.\"\n",
        "ngram_list = generate_spacy_ngrams(text, n)\n",
        "for gram in ngram_list:\n",
        "    print(gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBomAlyo3Ius",
        "outputId": "71a2da53-d703-47af-bfce-24e9fda4940b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample\n",
            "is a sample sentence\n",
            "a sample sentence for\n",
            "sample sentence for N\n",
            "sentence for N-\n",
            "for N-gram\n",
            "N-gram implementation\n",
            "-gram implementation using\n",
            "gram implementation using spaCy\n",
            "implementation using spaCy and\n",
            "using spaCy and NLTK\n",
            "spaCy and NLTK.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementing N-grams in NLTK"
      ],
      "metadata": {
        "id": "yPA2cHxA63mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"This is a sample sentence for N-gram implementation using spaCy and NLTK.\"\n",
        "tokens = word_tokenize(text)\n",
        "n = 2\n",
        "n_grams = list(ngrams(tokens,n))\n",
        "for token in n_grams:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO1QAU2t4U-p",
        "outputId": "048a4c78-5355-45b0-de3c-d651daae3a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('This', 'is')\n",
            "('is', 'a')\n",
            "('a', 'sample')\n",
            "('sample', 'sentence')\n",
            "('sentence', 'for')\n",
            "('for', 'N-gram')\n",
            "('N-gram', 'implementation')\n",
            "('implementation', 'using')\n",
            "('using', 'spaCy')\n",
            "('spaCy', 'and')\n",
            "('and', 'NLTK')\n",
            "('NLTK', '.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "def ntlk_ngrams(text,n):\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [token.lower() for token in tokens if token not in string.punctuation]\n",
        "  n_grams = list(ngrams(tokens,n))\n",
        "  return n_grams\n",
        "\n",
        "text = \"This is a sample sentence for N-gram implementation using spaCy and NLTK.\"\n",
        "n = 4\n",
        "\n",
        "n_list = ntlk_ngrams(text, n)\n",
        "for gram in n_list:\n",
        "  print(gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15-8KhqE9H-T",
        "outputId": "d4c6f046-235c-4fb6-c16e-13135004be2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('this', 'is', 'a', 'sample')\n",
            "('is', 'a', 'sample', 'sentence')\n",
            "('a', 'sample', 'sentence', 'for')\n",
            "('sample', 'sentence', 'for', 'n-gram')\n",
            "('sentence', 'for', 'n-gram', 'implementation')\n",
            "('for', 'n-gram', 'implementation', 'using')\n",
            "('n-gram', 'implementation', 'using', 'spacy')\n",
            "('implementation', 'using', 'spacy', 'and')\n",
            "('using', 'spacy', 'and', 'nltk')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NER using spaCy"
      ],
      "metadata": {
        "id": "HeaOx25b1nVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\"\n",
        "doc=nlp(text)\n",
        "# Iterate through the named entities and print them\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Type: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6jSWVJs-sS3",
        "outputId": "b6ad1fe7-203b-4604-93bd-7d4c6f092573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Apple Inc., Type: ORG\n",
            "Entity: Steve Jobs, Type: PERSON\n",
            "Entity: Cupertino, Type: GPE\n",
            "Entity: California, Type: GPE\n",
            "Entity: 1976, Type: DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the NLTK package and the named entity recognition corpus\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform NER\n",
        "entities = nltk.chunk.ne_chunk(nltk.pos_tag(words))\n",
        "\n",
        "# Iterate through the named entities and print them\n",
        "for subtree in entities:\n",
        "    if type(subtree) == nltk.Tree:\n",
        "        entity = \" \".join([word for word, tag in subtree.leaves()])\n",
        "        label = subtree.label()\n",
        "        print(f\"Entity: {entity}, Type: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w7Ts_iw11tz",
        "outputId": "bad49c45-260b-464c-cc10-11dcf9f430f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Apple, Type: PERSON\n",
            "Entity: Inc., Type: ORGANIZATION\n",
            "Entity: Steve Jobs, Type: PERSON\n",
            "Entity: Cupertino, Type: GPE\n",
            "Entity: California, Type: GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Corpus using Spacy\n"
      ],
      "metadata": {
        "id": "bBJpS5Om4lii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text data (you can add more text)\n",
        "texts = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"Here's another sentence.\",\n",
        "    \"And this is the third one.\",\n",
        "]\n",
        "\n",
        "# Process the text with spaCy\n",
        "corpus = [doc.text for doc in nlp.pipe(texts)]\n",
        "\n",
        "# Print the corpus\n",
        "for document in corpus:\n",
        "    print(document)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7cse1FH2C3l",
        "outputId": "8776101c-163a-4462-d655-e1400e444332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first sentence.\n",
            "Here's another sentence.\n",
            "And this is the third one.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of words"
      ],
      "metadata": {
        "id": "8dNZ2u2SEOqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text data\n",
        "text = \"This is a sample sentence. You can add more sentences for testing.\"\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize and preprocess the text\n",
        "tokens = [token.text.lower() for token in doc if not token.is_punct]\n",
        "bow = {}\n",
        "for token in tokens:\n",
        "    if token in bow:\n",
        "        bow[token] += 1\n",
        "    else:\n",
        "        bow[token] = 1\n",
        "print(\"Bag of Words:\")\n",
        "for word, frequency in bow.items():\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzao0bpqCwnC",
        "outputId": "a6fd93d2-b573-47b5-a953-5e94ffcbb666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            "this: 1\n",
            "is: 1\n",
            "a: 1\n",
            "sample: 1\n",
            "sentence: 1\n",
            "you: 1\n",
            "can: 1\n",
            "add: 1\n",
            "more: 1\n",
            "sentences: 1\n",
            "for: 1\n",
            "testing: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "# Sample text data\n",
        "text = \"This is a sample sentence. You can add more sentences for testing.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalnum()]\n",
        "\n",
        "# Create the Bag of Words\n",
        "bow = {}\n",
        "for word in filtered_words:\n",
        "    if word in bow:\n",
        "        bow[word] += 1\n",
        "    else:\n",
        "        bow[word] = 1\n",
        "\n",
        "# Print the Bag of Words\n",
        "print(\"Bag of Words:\")\n",
        "for word, frequency in bow.items():\n",
        "    print(f\"{word}: {frequency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z6zZ3ybGFsz",
        "outputId": "c23d8f5a-5f4b-4eb0-d8ca-0e8752cd6595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            "sample: 1\n",
            "sentence: 1\n",
            "add: 1\n",
            "sentences: 1\n",
            "testing: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF-spacy"
      ],
      "metadata": {
        "id": "ktp3Wrr5GqA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text data (list of documents)\n",
        "texts = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "# Tokenize and preprocess the text\n",
        "processed_texts = []\n",
        "for text in texts:\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_punct]\n",
        "    processed_texts.append(\" \".join(tokens))\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Calculate TF-IDF scores\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "# Print TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# Print feature names (words)\n",
        "print(\"Feature Names (Words):\")\n",
        "print(feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qeq5BlLHGMv",
        "outputId": "4b7085b8-3419-4472-922a-8ff677ca09f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "Feature Names (Words):\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Td-IDF -- NLTK"
      ],
      "metadata": {
        "id": "xW85r8KfHRAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data (list of documents)\n",
        "texts = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Tokenize and preprocess the text\n",
        "processed_texts = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for text in texts:\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalnum()]\n",
        "    processed_texts.append(\" \".join(filtered_words))\n",
        "# Create a TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Calculate TF-IDF scores\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "# Print TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# Print feature names (words)\n",
        "print(\"Feature Names (Words):\")\n",
        "print(feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lujpHpn5HM71",
        "outputId": "3ebadad8-9c70-4010-ad95-edf4932c485b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.62922751 0.77722116 0.         0.         0.        ]\n",
            " [0.78722298 0.         0.         0.61666846 0.        ]\n",
            " [0.         0.         0.70710678 0.         0.70710678]\n",
            " [0.62922751 0.77722116 0.         0.         0.        ]]\n",
            "Feature Names (Words):\n",
            "['document' 'first' 'one' 'second' 'third']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "text = \"This is a sample sentence. Tokenization is important in NLP.\"\n",
        "text = text.lower()\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word, wordnet.VERB) for word in stemmed_tokens]\n"
      ],
      "metadata": {
        "id": "HcTdQ4aDHfTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e501cdc-1f26-4f05-aecb-1c914cfa9139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHRUj7EJ1vgX",
        "outputId": "ace431e2-ac52-4024-f8e3-29306a9398a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sampl', 'sentenc', '.', 'token', 'import', 'nlp', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD4oZPVN26Ef",
        "outputId": "e60a5ff4-4c00-454f-b5d1-ba3895af27a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sampl', 'sentenc', '.', 'token', 'import', 'nlp', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Text:\", text)\n",
        "print(\"Tokenized Text:\", tokens)\n",
        "print(\"Filtered Text (Stopword Removal):\", filtered_tokens)\n",
        "print(\"Stemmed Text:\", stemmed_tokens)\n",
        "print(\"Lemmatized Text:\", lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD_iZL0b3Kuw",
        "outputId": "a065c16e-7350-4589-91cc-b4b3f45a54e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: this is a sample sentence. tokenization is important in nlp.\n",
            "Tokenized Text: ['this', 'is', 'a', 'sample', 'sentence', '.', 'tokenization', 'is', 'important', 'in', 'nlp', '.']\n",
            "Filtered Text (Stopword Removal): ['sample', 'sentence', '.', 'tokenization', 'important', 'nlp', '.']\n",
            "Stemmed Text: ['sampl', 'sentenc', '.', 'token', 'import', 'nlp', '.']\n",
            "Lemmatized Text: ['sampl', 'sentenc', '.', 'token', 'import', 'nlp', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FA29DHXU3V4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}